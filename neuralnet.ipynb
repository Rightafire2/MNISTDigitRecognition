{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_train_images = 60000\n",
    "number_test_images = 10000\n",
    "total_pixels=784\n",
    "column_names=['labels'] + [f'pixel{i}' for i in range(total_pixels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Paths for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPath(relative_path):\n",
    "    # Create a Path object with the relative path and resolve it to an absolute path\n",
    "    return str(Path.cwd().joinpath(relative_path).resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_ubyte_images = getPath(\"data/train-images-idx3-ubyte\")\n",
    "mnist_train_ubyte_labels = getPath(\"data/train-labels-idx1-ubyte\")\n",
    "mnist_test_ubyte_images = getPath(\"data/t10k-images-idx3-ubyte\")\n",
    "mnist_test_ubyte_labels = getPath(\"data/t10k-labels-idx1-ubyte\")\n",
    "train_csv_path = getPath(\"mnist_csv/mnist_train.csv\")\n",
    "test_csv_path = getPath(\"mnist_csv/mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_to_csv(imgs, labels, csvPath, n, total_pixels):\n",
    "    # Read image and label dat\n",
    "    imgData = open(imgs, 'rb')\n",
    "    labelData = open(labels, 'rb')\n",
    "    # Write CSV file\n",
    "    csvData = open(csvPath, 'w')\n",
    "\n",
    "    # Skip header information and write to images array\n",
    "    imgData.read(16)\n",
    "    labelData.read(8)\n",
    "    images = []\n",
    "\n",
    "    # Loop through n and append pixel data to images array\n",
    "    for i in range(n):\n",
    "        # Capture label in image array and loop through total pixels count to add corresponding data to image\n",
    "        image = [ord(labelData.read(1))]\n",
    "        for j in range(total_pixels):\n",
    "            image.append(ord(imgData.read(1)))\n",
    "        # Append image list to images list\n",
    "        images.append(image)\n",
    "\n",
    "    # Write image data to CSV file by iterating through image array and separating row data by comma\n",
    "    for image in images:\n",
    "        csvData.write(','.join(str(pix) for pix in image) + '\\n')\n",
    "\n",
    "    # Close files\n",
    "    imgData.close()\n",
    "    labelData.close()\n",
    "    csvData.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_to_csv(mnist_train_ubyte_images, mnist_train_ubyte_labels, train_csv_path, number_train_images, total_pixels)\n",
    "mnist_to_csv(mnist_test_ubyte_images, mnist_test_ubyte_labels, test_csv_path, number_test_images, total_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(train_csv_path, header=None, names=column_names)\n",
    "df_test = pd.read_csv(test_csv_path, header=None, names=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a NeuralNetwork Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_layers, output_size, activation_functions, learning_rate=0.01):\n",
    "        self.input_size = input_size # First Layer Size (784 for 28x28 pixel images)\n",
    "        self.hidden_layers = hidden_layers # Array with Hidden Layers Sizes\n",
    "        self.output_size = output_size # Last Layer Size (10 for each digit in MNIST)\n",
    "        self.activation_functions = activation_functions # Array of Activation Functions for each layer\n",
    "        self.learning_rate = learning_rate # Learning Rate\n",
    "        self.layers = [self.input_size] + hidden_layers + [self.output_size] # Array Containing the Size of Each Layer\n",
    "        self.weights = self.initialize_weights() # Initialize weights\n",
    "        self.biases = self.initialize_biases() # Initializa bias\n",
    "        self.loss_history = [] # Save loss for plotting\n",
    "        self.accuracy_history = [] # Save accuracy for plotting\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # Simple Random Weights:\n",
    "        #   Create an array of 2D arrays of size Layer1 x Layer 2 for the sake of matrix multiplication\n",
    "        #   Weight Initialization Sizes Reference: https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/\n",
    "        return [np.random.randn(self.layers[i+1], self.layers[i]) * np.sqrt(2. / self.layers[i]) for i in range(len(self.layers)-1)]\n",
    "        \n",
    "\n",
    "    def initialize_biases(self):\n",
    "        # Bias:\n",
    "        #   Create a array of arrays of size Layer2 x 1\n",
    "        return [np.zeros((self.layers[i+1], 1)) for i in range(len(self.layers)-1)]\n",
    "\n",
    "    def forward_propagation(self, weights, biases, x, a_t=None, count=0):\n",
    "        # Forward Propagation:\n",
    "        #   Iterate through list of weights and biases through recursion\n",
    "        #   Base Case: When count has reached the length of the weights array there is no more data to apply forward propagation\n",
    "        #   In each iteration apply the formula wT*x+b to get z and apply the activation function specified for that layer\n",
    "        \n",
    "        if a_t is None:\n",
    "            a_t = [x]\n",
    "    \n",
    "        if count == len(weights):\n",
    "            return x, a_t\n",
    "    \n",
    "        z = np.dot(weights[count], x) + biases[count]\n",
    "        print(f\"Layer {count}: z mean = {np.mean(z)}, z min = {np.min(z)}, z max = {np.max(z)}\")  # Debugging\n",
    "        a_t.append(self.activation_functions[count](z))\n",
    "        return self.forward_propagation(weights, biases, a_t[-1], a_t, count+1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        # ReLU function\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        # Sigmoid function\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(z):\n",
    "        # Hyperbolic tan function\n",
    "        return (np.exp(z) - np.exp(-z))/(np.exp(z) + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        # Softmax function\n",
    "        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "    \n",
    "    def activation_derivative(self, activation, function):\n",
    "        # Derivative of activation functions\n",
    "        if function == self.sigmoid or function.__name__ == 'sigmoid':\n",
    "            return activation * (1 - activation)\n",
    "        elif function == self.relu or function.__name__ == 'relu':\n",
    "            return np.where(activation > 0, 1, 0)\n",
    "        elif function == self.tanh or function.__name__ == 'tanh':\n",
    "            return 1 - activation**2\n",
    "        elif function == self.softmax or function.__name__ == 'softmax':\n",
    "            return np.ones_like(activation)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation function\")\n",
    "\n",
    "\n",
    "    def backward_propagation(self, y_true, activations, weights, biases):\n",
    "        # Backwanrds Propagation:\n",
    "        #   Initialize weights/bias gradient lists\n",
    "        #   Calculate error delta of the last layer\n",
    "        #   Iterate backwards through the weights and biases and apply the formulas to determine the gradients\n",
    "        #   Update weights using formula w=w-n*dL/dw or b=b-n*dL/db\n",
    "\n",
    "        weight_grads = [None]*len(weights)\n",
    "        bias_grads = [None]*len(biases)\n",
    "        delta = activations[-1] - y_true\n",
    "    \n",
    "        for i in reversed(range(len(weights))):\n",
    "            weight_grads[i] = np.dot(delta, activations[i].T)\n",
    "            bias_grads[i] = np.sum(delta, axis=1, keepdims=True)\n",
    "        \n",
    "            if i != 0:\n",
    "                delta = np.dot(weights[i].T, delta) * self.activation_derivative(activations[i], self.activation_functions[i])\n",
    "\n",
    "        for i in range(len(weights)):\n",
    "            weights[i] -= self.learning_rate * weight_grads[i]\n",
    "            biases[i] -= self.learning_rate * bias_grads[i]\n",
    "\n",
    "        return weights, biases\n",
    "    \n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        # Cross-Entropy Loss\n",
    "        # Added extremely small number to avoid taking log(0)\n",
    "        m = y_true.shape[1]\n",
    "        epsilon = 1e-8  # Small value to prevent log(0)\n",
    "        loss = -np.sum(y_true * np.log(y_pred + epsilon)) / m\n",
    "        return loss\n",
    "\n",
    "    def train(self, x_train, y_train, x=None, y=None, epochs=10):\n",
    "        # Training:\n",
    "        #   Loop through each epoch\n",
    "        #   Forward propagate and calculate loss\n",
    "        #   Backwards propagate \n",
    "        for epoch in range(epochs):\n",
    "            y_pred, activations = self.forward_propagation(self.weights, self.biases, x_train)\n",
    "            loss = self.compute_loss(y_train, y_pred)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            if x is not None and y is not None:\n",
    "                y_val_pred, _ = self.forward_propagation(self.weights, self.biases, x)\n",
    "                val_accuracy = self.accuracy(y, y_val_pred)\n",
    "                self.accuracy_history.append(val_accuracy)\n",
    "            \n",
    "            self.weights, self.biases = self.backward_propagation(y_train, activations, self.weights, self.biases)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                if x is not None and y is not None:\n",
    "                    print(f'Epoch {epoch}, Loss: {loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "                else:\n",
    "                    print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Predict method to use the network\n",
    "        y_pred, _ = self.forward_propagation(self.weights, self.biases, x)\n",
    "        return np.argmax(y_pred, axis=0)\n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        # Accuracy calculation\n",
    "        predictions = np.argmax(y_pred, axis=0)\n",
    "        return np.mean(predictions == y_true)\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        # Plotting the loss curve\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.loss_history, label='Loss')\n",
    "        plt.title('Loss over epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        if self.accuracy_history:\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(self.accuracy_history, label='Validation Accuracy', color='orange')\n",
    "            plt.title('Validation Accuracy over epochs')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your data frames are named train_df and test_df\n",
    "\n",
    "# Separate features and labels\n",
    "x_train = df_train.drop('labels', axis=1).values  # Drop the 'label' column\n",
    "y_train = df_train['labels'].values  # Extract the 'label' column\n",
    "\n",
    "x_test = df_test.drop('labels', axis=1).values\n",
    "y_test = df_test['labels'].values\n",
    "\n",
    "# Normalize the pixel values to [0, 1]\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Transpose the data to match the input format expected by the neural network\n",
    "x_train = x_train.T\n",
    "x_test = x_test.T\n",
    "\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_train = encoder.fit_transform(y_train.reshape(-1, 1)).T\n",
    "y_test = encoder.transform(y_test.reshape(-1, 1)).T\n",
    "\n",
    "# Initialize the neural network\n",
    "nn = NeuralNetwork(input_size=784, hidden_layers=[128, 64], output_size=10,\n",
    "                   activation_functions=[NeuralNetwork.relu, NeuralNetwork.relu, NeuralNetwork.softmax], learning_rate=0.01)\n",
    "\n",
    "# Train the network\n",
    "nn.train(x_train, y_train, epochs=100)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_test = nn.predict(x_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = nn.accuracy(np.argmax(y_test, axis=0), y_pred_test)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Plot the training metrics\n",
    "nn.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_layers, output_size, activation_functions, learning_rate=0.0001):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.output_size = output_size\n",
    "        self.activation_functions = activation_functions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layers = [self.input_size] + hidden_layers + [self.output_size]\n",
    "        self.weights = self.initialize_weights()\n",
    "        self.biases = self.initialize_biases()\n",
    "        self.loss_history = []\n",
    "        self.accuracy_history = []\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        return [np.random.randn(self.layers[i+1], self.layers[i]) * np.sqrt(1. / self.layers[i]) for i in range(len(self.layers)-1)]\n",
    "\n",
    "    def initialize_biases(self):\n",
    "        return [np.zeros((self.layers[i+1], 1)) for i in range(len(self.layers)-1)]\n",
    "\n",
    "    def forward_propagation(self, weights, biases, x, a_t=None, count=0):\n",
    "        if a_t is None:\n",
    "            a_t = [x]\n",
    "        \n",
    "        if count == len(weights):\n",
    "            return x, a_t\n",
    "        \n",
    "        z = np.dot(weights[count], x) + biases[count]\n",
    "        print(f\"Layer {count}: z mean = {np.mean(z)}, z min = {np.min(z)}, z max = {np.max(z)}\")\n",
    "        a_t.append(self.activation_functions[count](z))\n",
    "        return self.forward_propagation(weights, biases, a_t[-1], a_t, count + 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def leaky_relu(z, alpha=0.01):\n",
    "        return np.where(z > 0, z, alpha * z)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(z):\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "\n",
    "    def activation_derivative(self, activation, function):\n",
    "        if function == self.sigmoid or function.__name__ == 'sigmoid':\n",
    "            return activation * (1 - activation)\n",
    "        elif function == self.leaky_relu or function.__name__ == 'leaky_relu':\n",
    "            return np.where(activation > 0, 1, 0.01)\n",
    "        elif function == self.tanh or function.__name__ == 'tanh':\n",
    "            return 1 - activation**2\n",
    "        elif function == self.softmax or function.__name__ == 'softmax':\n",
    "            return np.ones_like(activation)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation function\")\n",
    "\n",
    "    def backward_propagation(self, y_true, activations, weights, biases):\n",
    "        weight_grads = [None]*len(weights)\n",
    "        bias_grads = [None]*len(biases)\n",
    "        delta = activations[-1] - y_true\n",
    "\n",
    "        for i in reversed(range(len(weights))):\n",
    "            weight_grads[i] = np.dot(delta, activations[i].T)\n",
    "            bias_grads[i] = np.sum(delta, axis=1, keepdims=True)\n",
    "\n",
    "            # Apply gradient clipping\n",
    "            weight_grads[i] = np.clip(weight_grads[i], -1, 1)\n",
    "            bias_grads[i] = np.clip(bias_grads[i], -1, 1)\n",
    "        \n",
    "            if i != 0:\n",
    "                delta = np.dot(weights[i].T, delta) * self.activation_derivative(activations[i], self.activation_functions[i])\n",
    "\n",
    "        for i in range(len(weights)):\n",
    "            weights[i] -= self.learning_rate * weight_grads[i]\n",
    "            biases[i] -= self.learning_rate * bias_grads[i]\n",
    "\n",
    "        return weights, biases\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        m = y_true.shape[1]\n",
    "        epsilon = 1e-8  # Small value to prevent log(0)\n",
    "        loss = -np.sum(y_true * np.log(y_pred + epsilon)) / m\n",
    "        return loss\n",
    "\n",
    "    def train(self, x_train, y_train, x=None, y=None, epochs=100):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred, activations = self.forward_propagation(self.weights, self.biases, x_train)\n",
    "            loss = self.compute_loss(y_train, y_pred)\n",
    "            self.loss_history.append(loss)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            train_accuracy = self.accuracy(np.argmax(y_train, axis=0), np.argmax(y_pred, axis=0))\n",
    "            self.accuracy_history.append(train_accuracy)\n",
    "        \n",
    "            if x is not None and y is not None:\n",
    "                y_val_pred, _ = self.forward_propagation(self.weights, self.biases, x)\n",
    "                val_accuracy = self.accuracy(y, y_val_pred)\n",
    "                self.accuracy_history.append(val_accuracy)\n",
    "        \n",
    "            self.weights, self.biases = self.backward_propagation(y_train, activations, self.weights, self.biases)\n",
    "        \n",
    "            if epoch % 10 == 0:\n",
    "                if x is not None and y is not None:\n",
    "                    print(f'Epoch {epoch}, Loss: {loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "                else:\n",
    "                    print(f'Epoch {epoch}, Loss: {loss:.4f}, Training Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_pred, _ = self.forward_propagation(self.weights, self.biases, x)\n",
    "        return np.argmax(y_pred, axis=0)\n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        predictions = np.argmax(y_pred, axis=0)\n",
    "        return np.mean(predictions == y_true)\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        plt.figure(figsize=(14, 7))\n",
    "    \n",
    "        # Plot Loss\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(self.loss_history, label='Loss')\n",
    "        plt.title('Loss over epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "    \n",
    "        # Plot Accuracy\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(self.accuracy_history, label='Training Accuracy', color='green')\n",
    "        plt.title('Accuracy over epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "x_train = df_train.drop('labels', axis=1).values  # Drop the 'label' column\n",
    "y_train = df_train['labels'].values  # Extract the 'label' column\n",
    "\n",
    "x_test = df_test.drop('labels', axis=1).values\n",
    "y_test = df_test['labels'].values\n",
    "\n",
    "# Normalize the pixel values to [0, 1]\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Transpose the data to match the input format expected by the neural network\n",
    "x_train = x_train.T\n",
    "x_test = x_test.T\n",
    "\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_train = encoder.fit_transform(y_train.reshape(-1, 1)).T\n",
    "y_test = encoder.transform(y_test.reshape(-1, 1)).T\n",
    "\n",
    "# Initialize the neural network with the Leaky ReLU activation function and a lower learning rate\n",
    "nn = NeuralNetwork(input_size=784, hidden_layers=[128, 64], output_size=10,\n",
    "                   activation_functions=[NeuralNetwork.leaky_relu, NeuralNetwork.leaky_relu, NeuralNetwork.softmax],\n",
    "                   learning_rate=0.0001)\n",
    "\n",
    "# Train the network\n",
    "nn.train(x_train, y_train, epochs=1000)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_test = nn.predict(x_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = nn.accuracy(np.argmax(y_test, axis=0), y_pred_test)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Plot the training metrics\n",
    "nn.plot_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
